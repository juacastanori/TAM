{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# â”€â”€â”€ INSTALACIONES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n!pip install vizdoom==1.2.4 pyvirtualdisplay imageio imageio-ffmpeg opencv-python-headless stable-baselines3 --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio\nimport cv2\n\nfrom pyvirtualdisplay import Display\nfrom collections import deque\nfrom IPython.display import Video, display\n\nimport torch as th\nimport torch.nn as nn\n\nimport vizdoom as vzd\n\nimport gym\nfrom gym.wrappers import FrameStack\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecTransposeImage, VecFrameStack\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# â”€â”€â”€ CONFIGURACIÃ“N GPU Y DISPLAY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndevice = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\nDisplay(visible=0, size=(640, 480)).start()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€ DESCARGA DEL ESCENARIO PERSONALIZADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n!wget -q https://raw.githubusercontent.com/lkiel/rl-doom/develop/scenarios/bots_deathmatch_multimaps.cfg -O bots_deathmatch_multimaps.cfg\n!wget -q https://raw.githubusercontent.com/lkiel/rl-doom/develop/scenarios/bots_deathmatch_multimaps.wad -O bots_deathmatch_multimaps.wad\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€ ARCHIVO DE CONFIGURACIÃ“N PERSONALIZADA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nwith open(\"reward_shaped.cfg\", \"w\") as f:\n    f.write(\"\"\"\nepisode_timeout = 4096\nscreen_resolution = RES_160X120\nscreen_format = RGB24\nrender_hud = false\nrender_weapon = false\nrender_crosshair = false\nwindow_visible = false\n\navailable_buttons =\n    {\n        ATTACK\n        MOVE_FORWARD\n        TURN_LEFT\n        TURN_RIGHT\n        MOVE_LEFT\n        MOVE_RIGHT\n    }\n\navailable_game_variables =\n    {\n        KILLCOUNT\n        DAMAGECOUNT\n        SELECTED_WEAPON\n        SELECTED_WEAPON_AMMO\n        HEALTH\n        ARMOR\n        POSITION_X\n        POSITION_Y\n    }\n\nmode = PLAYER\n\"\"\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â”€â”€â”€ FUNCIÃ“N PARA INICIALIZAR EL JUEGO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef create_game():\n    game = vzd.DoomGame()\n    game.load_config(\"reward_shaped.cfg\")\n    game.set_doom_scenario_path(\"bots_deathmatch_multimaps.wad\")\n    game.set_doom_map(\"M\")\n    game.add_game_args(\"-host 1 -deathmatch +sv_spawnfarthest 1 +viz_nocheat 0 +sv_forcerespawn 1\")\n    game.set_mode(vzd.Mode.PLAYER)\n    game.init()\n    game.send_game_command(\"removebots\")\n    for _ in range(4):\n        game.send_game_command(\"addbot\")\n    return game","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:48:20.875369Z","iopub.execute_input":"2025-07-23T00:48:20.875703Z","iopub.status.idle":"2025-07-23T00:48:20.885440Z","shell.execute_reply.started":"2025-07-23T00:48:20.875673Z","shell.execute_reply":"2025-07-23T00:48:20.884578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoomShapedEnv(gym.Env):\n    \"\"\"\n    Entorno Gym personalizado para ViZDoom con reward shaping.\n    ObservaciÃ³n: imagen RGB 160x120 (canal primero).\n    AcciÃ³n: una de 14 combinaciones discretas.\n    \"\"\"\n    def __init__(self, frame_skip=1):\n        super().__init__()\n        self.frame_skip = frame_skip\n        self.game = create_game()\n        self.game.new_episode()\n\n        # ObservaciÃ³n: (C, H, W)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255, shape=(3, 120, 160), dtype=np.uint8\n        )\n\n        # Definimos 14 combinaciones de acciones manuales\n        self.actions = [\n            [1, 0, 0, 0, 0, 0],  # atacar\n            [0, 1, 0, 0, 0, 0],  # avanzar\n            [0, 0, 1, 0, 0, 0],  # girar izquierda\n            [0, 0, 0, 1, 0, 0],  # girar derecha\n            [0, 0, 0, 0, 1, 0],  # moverse izquierda\n            [0, 0, 0, 0, 0, 1],  # moverse derecha\n            [0, 1, 1, 0, 0, 0],  # avanzar + girar izq\n            [0, 1, 0, 1, 0, 0],  # avanzar + girar der\n            [1, 0, 1, 0, 0, 0],  # atacar + girar izq\n            [1, 0, 0, 1, 0, 0],  # atacar + girar der\n            [1, 1, 0, 0, 0, 0],  # atacar + avanzar\n            [1, 0, 0, 0, 1, 0],  # atacar + moverse izq\n            [1, 0, 0, 0, 0, 1],  # atacar + moverse der\n            [0, 0, 0, 0, 0, 0],  # Nada\n        ]\n        self.action_space = gym.spaces.Discrete(len(self.actions))\n\n        # InicializaciÃ³n de variables internas\n        self.last_health = 100\n        self.last_armor = 0\n        self.last_pos = (\n            self.game.get_game_variable(vzd.GameVariable.POSITION_X),\n            self.game.get_game_variable(vzd.GameVariable.POSITION_Y)\n        )\n        self.last_damage = 0\n        self.ammo_state = self._get_ammo_state()\n        self.weapon_state = self._get_weapon_state()\n        self.last_kills = self.game.get_game_variable(vzd.GameVariable.KILLCOUNT)\n        self.last_weapon_id = self.game.get_game_variable(vzd.GameVariable.SELECTED_WEAPON)\n\n    def _get_ammo_state(self):\n        ammo = np.zeros(10, dtype=np.int32)\n        for i in range(10):\n            ammo[i] = self.game.get_game_variable(getattr(vzd.GameVariable, f\"AMMO{i}\"))\n        return ammo\n\n    def _get_weapon_state(self):\n        weapons = np.zeros(10, dtype=np.int32)\n        for i in range(10):\n            weapons[i] = self.game.get_game_variable(getattr(vzd.GameVariable, f\"WEAPON{i}\"))\n        return weapons\n\n    def step(self, action_idx):\n        if self.game.is_episode_finished():\n            obs = self.reset()\n            return obs, 0.0, True, {}\n\n        action = self.actions[action_idx]\n        reward = self.game.make_action(action, self.frame_skip)\n        done = self.game.is_episode_finished()\n        state = self.game.get_state()\n        obs = np.zeros(self.observation_space.shape, dtype=np.uint8) if state is None else np.transpose(state.screen_buffer, (2, 0, 1))\n\n        # Variables actuales\n        health = self.game.get_game_variable(vzd.GameVariable.HEALTH)\n        armor = self.game.get_game_variable(vzd.GameVariable.ARMOR)\n        damage = self.game.get_game_variable(vzd.GameVariable.DAMAGECOUNT)\n        killcount = self.game.get_game_variable(vzd.GameVariable.KILLCOUNT)\n        weapon_id = self.game.get_game_variable(vzd.GameVariable.SELECTED_WEAPON)\n        x = self.game.get_game_variable(vzd.GameVariable.POSITION_X)\n        y = self.game.get_game_variable(vzd.GameVariable.POSITION_Y)\n        is_alive = health > 0\n\n        shaped_reward = reward\n\n        if is_alive:\n            # Cambio de arma\n            extra_weapon_change = 0.05 if weapon_id != self.last_weapon_id else 0.0\n\n            # DaÃ±o infligido\n            damage_delta = damage - self.last_damage\n            extra_damage = 0.01 * damage_delta\n\n            # MuniciÃ³n (solo si no cambiÃ³ de arma)\n            new_ammo = self._get_ammo_state()\n            if weapon_id == self.last_weapon_id:\n                ammo_diff = (new_ammo - self.ammo_state) * self.weapon_state\n                pickup = np.clip(ammo_diff, 0, None).sum()\n                usage = -np.clip(ammo_diff, None, 0).sum()\n                extra_ammo = 0.02 * pickup + 0.01 * usage\n            else:\n                extra_ammo = 0.0  # no se considera si cambia de arma\n\n            # Salud\n            if self.last_health > 0:\n                health_gain = max(0, health - self.last_health)\n                health_loss = -min(0, health - self.last_health)\n            else:\n                health_gain = 0\n                health_loss = 0\n            extra_health = 0.02 * health_gain - 0.01 * health_loss\n\n            # Armadura\n            armor_gain = max(0, armor - self.last_armor)\n            extra_armor = 0.01 * armor_gain\n\n            # Movimiento\n            dist = np.sqrt((x - self.last_pos[0])**2 + (y - self.last_pos[1])**2)\n            extra_move = 0.0005 if dist > 3 else -0.0025\n\n            # Kills\n            kill_delta = killcount - self.last_kills\n            extra_kill = 1.0 * max(0, kill_delta)\n\n            # Sumar todo\n            shaped_reward += (\n                extra_damage + extra_ammo + extra_health + extra_armor +\n                extra_move + extra_kill + extra_weapon_change\n            )\n\n        else:\n            # EstÃ¡ muerto: solo se cuentan kills si ocurrieron justo antes de morir\n            kill_delta = killcount - self.last_kills\n            shaped_reward += 1.0 * max(0, kill_delta)\n            new_ammo = self._get_ammo_state()  # actualizar de todos modos\n\n        # ActualizaciÃ³n de estado\n        self.last_health = health\n        self.last_armor = armor\n        self.last_pos = (x, y)\n        self.last_damage = damage\n        self.last_kills = killcount\n        self.ammo_state = new_ammo\n        self.last_weapon_id = weapon_id\n\n        return obs, shaped_reward, done, {}\n\n    def reset(self):\n        self.game.new_episode()\n        self.game.send_game_command(\"removebots\")\n        for _ in range(4):\n            self.game.send_game_command(\"addbot\")\n        self.last_health = 100\n        self.last_armor = 0\n        self.last_pos = (\n            self.game.get_game_variable(vzd.GameVariable.POSITION_X),\n            self.game.get_game_variable(vzd.GameVariable.POSITION_Y)\n        )\n        self.last_damage = 0\n        self.ammo_state = self._get_ammo_state()\n        self.weapon_state = self._get_weapon_state()\n        self.last_kills = self.game.get_game_variable(vzd.GameVariable.KILLCOUNT)\n        self.last_weapon_id = self.game.get_game_variable(vzd.GameVariable.SELECTED_WEAPON)\n        state = self.game.get_state()\n        return np.zeros(self.observation_space.shape, dtype=np.uint8) if state is None else np.transpose(state.screen_buffer, (2, 0, 1))\n\n    def render(self, mode='rgb_array'):\n            # Render del entorno devolviendo la imagen actual (H, W, C)\n            state = self.game.get_state()\n            if state is None:\n                return np.zeros((120, 160, 3), dtype=np.uint8)\n            return state.screen_buffer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nclass CustomCNN(BaseFeaturesExtractor):\n    \"\"\"\n    Red convolucional personalizada para extraer caracterÃ­sticas de la pantalla de Doom.\n    Arquitectura: 3 capas conv (ReLU) + capa lineal final.\n    \"\"\"\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        \n        n_channels = observation_space.shape[0]  # 12 si usas FrameStack(4)\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n            nn.Flatten()\n        )\n\n        # Determinar tamaÃ±o de la salida de la CNN\n        with th.no_grad():\n            sample_input = th.zeros(1, *observation_space.shape)\n            n_flatten = self.cnn(sample_input).shape[1]\n\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        x = observations / 255.0  # NormalizaciÃ³n [0,1]\n        x = self.cnn(x)\n        return self.linear(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack\n\n# NÃºmero de entornos paralelos\nnum_envs = 8\n\n# FrameStack = 4 (para apilar 4 imÃ¡genes consecutivas)\nframe_stack_n = 4\n\n# FunciÃ³n para crear un entorno individual (usada por cada subproceso)\ndef make_env():\n    def _init():\n        env = DoomShapedEnv(frame_skip=1)  # sin frame_skip ahora\n        return env\n    return _init\n\n# Crear mÃºltiples entornos paralelos\nenv_train = SubprocVecEnv([make_env() for _ in range(num_envs)])\n\n# Apilar 4 frames consecutivos (para dar sentido del movimiento)\nenv_train = VecFrameStack(env_train, n_stack=frame_stack_n, channels_order='first')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n\n# Listas globales\nepisode_rewards = []\nepisode_rewards2 = []\nepisode_losses = []\n\nclass LoggingCallback(BaseCallback):\n    def __init__(self, verbose=0, save_path=\"best_models_inline\", reward_threshold=50.0):\n        super().__init__(verbose)\n        self.current_rewards = None\n        self.best_reward = -np.inf\n        self.save_path = save_path\n        self.reward_threshold = reward_threshold\n        os.makedirs(self.save_path, exist_ok=True)\n\n    def _on_training_start(self) -> None:\n        self.current_rewards = [0.0] * self.training_env.num_envs\n        \n    def _on_step(self) -> bool:\n        rewards = self.locals[\"rewards\"]\n        dones = self.locals[\"dones\"]\n\n        for i in range(len(rewards)):\n            self.current_rewards[i] += rewards[i]\n            if dones[i]:\n                total = self.current_rewards[i]\n                episode_rewards2.append(total)\n                print(f\"Episodio completado: reward = {total:.2f}\")\n\n                # Guardar modelo si reward supera el umbral y es el mejor\n                if total >= self.reward_threshold and total > self.best_reward:\n                    self.best_reward = total\n                    filename = f\"{self.save_path}/best_model_ep{self.num_timesteps}_rew{total:.1f}\".replace(\".\", \"_\")\n                    self.model.save(filename)\n                    print(f\"ğŸ’¾ Guardado nuevo mejor modelo con reward = {total:.2f} en {filename}.zip\")\n\n                self.current_rewards[i] = 0.0\n\n        # Registrar pÃ©rdida (entropÃ­a del actor)\n        maybe_loss = self.model.logger.name_to_value.get(\"train/entropy_loss\")\n        if maybe_loss is not None:\n            episode_losses.append(maybe_loss)\n\n        return True\n\n    def _on_rollout_end(self) -> None:\n        if \"rollout/ep_rew_mean\" in self.model.logger.name_to_value:\n            rew = self.model.logger.name_to_value[\"rollout/ep_rew_mean\"]\n            episode_rewards.append(rew)\n            print(f\"Recompensa en rollout completado: {rew:.2f}\")\n\n\n# â”€â”€â”€ CALLBACK DE CHECKPOINTS CADA 500K PASOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ncheckpoint_callback = CheckpointCallback(\n    save_freq=500_000,\n    save_path=\"./checkpoints\",\n    name_prefix=\"ppo_doom_phase1\"\n)\n\n# â”€â”€â”€ DEFINICIÃ“N DEL MODELO PPO CON CNN PERSONALIZADA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n    features_extractor_kwargs=dict(features_dim=512)\n)\n\nmodel = PPO(\n    policy=\"CnnPolicy\",\n    env=env_train,\n    learning_rate=1e-4,\n    n_steps=1024,\n    batch_size=1024,\n    n_epochs=4,\n    ent_coef=0.02,  # â† Aumenta la entropÃ­a para explorar mÃ¡s\n    policy_kwargs=policy_kwargs,\n    verbose=1,\n    device=device\n)\n\nlogging_callback = LoggingCallback(save_path=\"best_models_inline\", reward_threshold=50.0)\n\nmodel.learn(\n    total_timesteps=9_000_000,\n    callback=[checkpoint_callback, logging_callback]\n)\n\n\n# â”€â”€â”€ GUARDAR MODELO ENTRENADO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel.save(\"ppo_doom_bots_phase1\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\n# â”€â”€â”€ GRÃFICO DE REWARD POR EPISODIO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nplt.subplot(1, 2, 1)\nplt.plot(episode_rewards)\nplt.title(\"Recompensa por rollout (no promedio mÃ³vil)\")\nplt.xlabel(\"Rollout (cada n_steps pasos)\")\nplt.ylabel(\"Recompensa\")\nplt.grid(True)\n\n# â”€â”€â”€ GRÃFICO DE ENTROPÃA (PROXY DE PÃ‰RDIDA) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nplt.subplot(1, 2, 2)\nplt.plot(episode_losses)\nplt.title(\"EntropÃ­a del actor (proxy de pÃ©rdida)\")\nplt.xlabel(\"Paso de entrenamiento\")\nplt.ylabel(\"EntropÃ­a\")\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 4))\nplt.plot(episode_rewards2)\nplt.title(\"Recompensa por episodio\")\nplt.xlabel(\"Episodio\")\nplt.ylabel(\"Reward total\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_and_record(model, filename=\"doom_phase1_eval.mp4\", num_episodes=5):\n    from IPython.display import Video, display\n    import time\n\n    env_eval = DummyVecEnv([make_env()])\n    env_eval = VecFrameStack(env_eval, n_stack=4, channels_order='first')\n    env_eval.venv.render_mode = \"rgb_array\"\n    \n    writer = imageio.get_writer(filename, fps=15)  # Puedes ajustar el fps aquÃ­\n    rewards = []\n\n    for ep in range(num_episodes):\n        obs = env_eval.reset()\n        done = [False]\n        total_reward = 0.0\n\n        while not done[0]:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, _ = env_eval.step(action)\n            total_reward += reward[0]\n\n            # Render del entorno\n            frame = env_eval.render(mode='rgb_array')\n            if frame is not None:\n                writer.append_data(frame)\n\n        print(f\"Episodio {ep+1}: reward = {total_reward:.2f}\")\n        rewards.append(total_reward)\n\n    writer.close()\n    print(\"Recompensas por episodio:\", rewards)\n    display(Video(filename, embed=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_and_record(model, filename=\"doom_phase1_eval.mp4\", num_episodes=5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}