{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ─── INSTALACIONES ─────────────────────────────────────────────────────────────\n!pip install vizdoom==1.2.4 pyvirtualdisplay imageio imageio-ffmpeg opencv-python-headless stable-baselines3 --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── IMPORTS ───────────────────────────────────────────────────────────────────\nimport os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio\nimport cv2\n\nfrom pyvirtualdisplay import Display\nfrom collections import deque\nfrom IPython.display import Video, display\n\nimport torch as th\nimport torch.nn as nn\n\nimport vizdoom as vzd\n\nimport gym\nfrom gym.wrappers import FrameStack\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecTransposeImage, VecFrameStack\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\n# ─── CONFIGURACIÓN GPU Y DISPLAY ───────────────────────────────────────────────\ndevice = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\nDisplay(visible=0, size=(640, 480)).start()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── DESCARGA DEL ESCENARIO PERSONALIZADO ──────────────────────────────────────\n!wget -q https://raw.githubusercontent.com/lkiel/rl-doom/develop/scenarios/bots_deathmatch_multimaps.cfg -O bots_deathmatch_multimaps.cfg\n!wget -q https://raw.githubusercontent.com/lkiel/rl-doom/develop/scenarios/bots_deathmatch_multimaps.wad -O bots_deathmatch_multimaps.wad\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── ARCHIVO DE CONFIGURACIÓN PERSONALIZADA ────────────────────────────────────\nwith open(\"reward_shaped.cfg\", \"w\") as f:\n    f.write(\"\"\"\nepisode_timeout = 4096\nscreen_resolution = RES_160X120\nscreen_format = RGB24\nrender_hud = false\nrender_weapon = false\nrender_crosshair = false\nwindow_visible = false\n\navailable_buttons =\n    {\n        ATTACK\n        MOVE_FORWARD\n        TURN_LEFT\n        TURN_RIGHT\n        MOVE_LEFT\n        MOVE_RIGHT\n    }\n\navailable_game_variables =\n    {\n        KILLCOUNT\n        DAMAGECOUNT\n        SELECTED_WEAPON\n        SELECTED_WEAPON_AMMO\n        HEALTH\n        ARMOR\n        POSITION_X\n        POSITION_Y\n    }\n\nmode = PLAYER\n\"\"\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── FUNCIÓN PARA INICIALIZAR EL JUEGO ─────────────────────────────────────────\ndef create_game():\n    game = vzd.DoomGame()\n    game.load_config(\"reward_shaped.cfg\")\n    game.set_doom_scenario_path(\"bots_deathmatch_multimaps.wad\")\n    game.set_doom_map(\"M\")\n    game.add_game_args(\"-host 1 -deathmatch +sv_spawnfarthest 1 +viz_nocheat 0 +sv_forcerespawn 1\")\n    game.set_mode(vzd.Mode.PLAYER)\n    game.init()\n    game.send_game_command(\"removebots\")\n    for _ in range(4):\n        game.send_game_command(\"addbot\")\n    return game","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:48:20.875369Z","iopub.execute_input":"2025-07-23T00:48:20.875703Z","iopub.status.idle":"2025-07-23T00:48:20.885440Z","shell.execute_reply.started":"2025-07-23T00:48:20.875673Z","shell.execute_reply":"2025-07-23T00:48:20.884578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoomShapedEnv(gym.Env):\n    \"\"\"\n    Entorno Gym personalizado para ViZDoom con reward shaping.\n    Observación: imagen RGB 160x120 (canal primero).\n    Acción: una de 14 combinaciones discretas.\n    \"\"\"\n    def __init__(self, frame_skip=1):\n        super().__init__()\n        self.frame_skip = frame_skip\n        self.game = create_game()\n        self.game.new_episode()\n\n        # Observación: (C, H, W)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255, shape=(3, 120, 160), dtype=np.uint8\n        )\n\n        # Definimos 14 combinaciones de acciones manuales\n        self.actions = [\n            [1, 0, 0, 0, 0, 0],  # atacar\n            [0, 1, 0, 0, 0, 0],  # avanzar\n            [0, 0, 1, 0, 0, 0],  # girar izquierda\n            [0, 0, 0, 1, 0, 0],  # girar derecha\n            [0, 0, 0, 0, 1, 0],  # moverse izquierda\n            [0, 0, 0, 0, 0, 1],  # moverse derecha\n            [0, 1, 1, 0, 0, 0],  # avanzar + girar izq\n            [0, 1, 0, 1, 0, 0],  # avanzar + girar der\n            [1, 0, 1, 0, 0, 0],  # atacar + girar izq\n            [1, 0, 0, 1, 0, 0],  # atacar + girar der\n            [1, 1, 0, 0, 0, 0],  # atacar + avanzar\n            [1, 0, 0, 0, 1, 0],  # atacar + moverse izq\n            [1, 0, 0, 0, 0, 1],  # atacar + moverse der\n            [0, 0, 0, 0, 0, 0],  # Nada\n        ]\n        self.action_space = gym.spaces.Discrete(len(self.actions))\n\n        # Inicialización de variables internas\n        self.last_health = 100\n        self.last_armor = 0\n        self.last_pos = (\n            self.game.get_game_variable(vzd.GameVariable.POSITION_X),\n            self.game.get_game_variable(vzd.GameVariable.POSITION_Y)\n        )\n        self.last_damage = 0\n        self.ammo_state = self._get_ammo_state()\n        self.weapon_state = self._get_weapon_state()\n        self.last_kills = self.game.get_game_variable(vzd.GameVariable.KILLCOUNT)\n        self.last_weapon_id = self.game.get_game_variable(vzd.GameVariable.SELECTED_WEAPON)\n\n    def _get_ammo_state(self):\n        ammo = np.zeros(10, dtype=np.int32)\n        for i in range(10):\n            ammo[i] = self.game.get_game_variable(getattr(vzd.GameVariable, f\"AMMO{i}\"))\n        return ammo\n\n    def _get_weapon_state(self):\n        weapons = np.zeros(10, dtype=np.int32)\n        for i in range(10):\n            weapons[i] = self.game.get_game_variable(getattr(vzd.GameVariable, f\"WEAPON{i}\"))\n        return weapons\n\n    def step(self, action_idx):\n        if self.game.is_episode_finished():\n            obs = self.reset()\n            return obs, 0.0, True, {}\n\n        action = self.actions[action_idx]\n        reward = self.game.make_action(action, self.frame_skip)\n        done = self.game.is_episode_finished()\n        state = self.game.get_state()\n        obs = np.zeros(self.observation_space.shape, dtype=np.uint8) if state is None else np.transpose(state.screen_buffer, (2, 0, 1))\n\n        # Variables actuales\n        health = self.game.get_game_variable(vzd.GameVariable.HEALTH)\n        armor = self.game.get_game_variable(vzd.GameVariable.ARMOR)\n        damage = self.game.get_game_variable(vzd.GameVariable.DAMAGECOUNT)\n        killcount = self.game.get_game_variable(vzd.GameVariable.KILLCOUNT)\n        weapon_id = self.game.get_game_variable(vzd.GameVariable.SELECTED_WEAPON)\n        x = self.game.get_game_variable(vzd.GameVariable.POSITION_X)\n        y = self.game.get_game_variable(vzd.GameVariable.POSITION_Y)\n        is_alive = health > 0\n\n        shaped_reward = reward\n\n        if is_alive:\n            # Cambio de arma\n            extra_weapon_change = 0.05 if weapon_id != self.last_weapon_id else 0.0\n\n            # Daño infligido\n            damage_delta = damage - self.last_damage\n            extra_damage = 0.01 * damage_delta\n\n            # Munición (solo si no cambió de arma)\n            new_ammo = self._get_ammo_state()\n            if weapon_id == self.last_weapon_id:\n                ammo_diff = (new_ammo - self.ammo_state) * self.weapon_state\n                pickup = np.clip(ammo_diff, 0, None).sum()\n                usage = -np.clip(ammo_diff, None, 0).sum()\n                extra_ammo = 0.02 * pickup + 0.01 * usage\n            else:\n                extra_ammo = 0.0  # no se considera si cambia de arma\n\n            # Salud\n            if self.last_health > 0:\n                health_gain = max(0, health - self.last_health)\n                health_loss = -min(0, health - self.last_health)\n            else:\n                health_gain = 0\n                health_loss = 0\n            extra_health = 0.02 * health_gain - 0.01 * health_loss\n\n            # Armadura\n            armor_gain = max(0, armor - self.last_armor)\n            extra_armor = 0.01 * armor_gain\n\n            # Movimiento\n            dist = np.sqrt((x - self.last_pos[0])**2 + (y - self.last_pos[1])**2)\n            extra_move = 0.0005 if dist > 3 else -0.0025\n\n            # Kills\n            kill_delta = killcount - self.last_kills\n            extra_kill = 1.0 * max(0, kill_delta)\n\n            # Sumar todo\n            shaped_reward += (\n                extra_damage + extra_ammo + extra_health + extra_armor +\n                extra_move + extra_kill + extra_weapon_change\n            )\n\n        else:\n            # Está muerto: solo se cuentan kills si ocurrieron justo antes de morir\n            kill_delta = killcount - self.last_kills\n            shaped_reward += 1.0 * max(0, kill_delta)\n            new_ammo = self._get_ammo_state()  # actualizar de todos modos\n\n        # Actualización de estado\n        self.last_health = health\n        self.last_armor = armor\n        self.last_pos = (x, y)\n        self.last_damage = damage\n        self.last_kills = killcount\n        self.ammo_state = new_ammo\n        self.last_weapon_id = weapon_id\n\n        return obs, shaped_reward, done, {}\n\n    def reset(self):\n        self.game.new_episode()\n        self.game.send_game_command(\"removebots\")\n        for _ in range(4):\n            self.game.send_game_command(\"addbot\")\n        self.last_health = 100\n        self.last_armor = 0\n        self.last_pos = (\n            self.game.get_game_variable(vzd.GameVariable.POSITION_X),\n            self.game.get_game_variable(vzd.GameVariable.POSITION_Y)\n        )\n        self.last_damage = 0\n        self.ammo_state = self._get_ammo_state()\n        self.weapon_state = self._get_weapon_state()\n        self.last_kills = self.game.get_game_variable(vzd.GameVariable.KILLCOUNT)\n        self.last_weapon_id = self.game.get_game_variable(vzd.GameVariable.SELECTED_WEAPON)\n        state = self.game.get_state()\n        return np.zeros(self.observation_space.shape, dtype=np.uint8) if state is None else np.transpose(state.screen_buffer, (2, 0, 1))\n\n    def render(self, mode='rgb_array'):\n            # Render del entorno devolviendo la imagen actual (H, W, C)\n            state = self.game.get_state()\n            if state is None:\n                return np.zeros((120, 160, 3), dtype=np.uint8)\n            return state.screen_buffer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nclass CustomCNN(BaseFeaturesExtractor):\n    \"\"\"\n    Red convolucional personalizada para extraer características de la pantalla de Doom.\n    Arquitectura: 3 capas conv (ReLU) + capa lineal final.\n    \"\"\"\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n        super(CustomCNN, self).__init__(observation_space, features_dim)\n        \n        n_channels = observation_space.shape[0]  # 12 si usas FrameStack(4)\n        \n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_channels, 32, kernel_size=8, stride=4), nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU(),\n            nn.Flatten()\n        )\n\n        # Determinar tamaño de la salida de la CNN\n        with th.no_grad():\n            sample_input = th.zeros(1, *observation_space.shape)\n            n_flatten = self.cnn(sample_input).shape[1]\n\n        self.linear = nn.Sequential(\n            nn.Linear(n_flatten, features_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, observations: th.Tensor) -> th.Tensor:\n        x = observations / 255.0  # Normalización [0,1]\n        x = self.cnn(x)\n        return self.linear(x)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack\n\n# Número de entornos paralelos\nnum_envs = 8\n\n# FrameStack = 4 (para apilar 4 imágenes consecutivas)\nframe_stack_n = 4\n\n# Función para crear un entorno individual (usada por cada subproceso)\ndef make_env():\n    def _init():\n        env = DoomShapedEnv(frame_skip=1)  # sin frame_skip ahora\n        return env\n    return _init\n\n# Crear múltiples entornos paralelos\nenv_train = SubprocVecEnv([make_env() for _ in range(num_envs)])\n\n# Apilar 4 frames consecutivos (para dar sentido del movimiento)\nenv_train = VecFrameStack(env_train, n_stack=frame_stack_n, channels_order='first')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n\n# Listas globales\nepisode_rewards = []\nepisode_rewards2 = []\nepisode_losses = []\n\nclass LoggingCallback(BaseCallback):\n    def __init__(self, verbose=0, save_path=\"best_models_inline\", reward_threshold=50.0):\n        super().__init__(verbose)\n        self.current_rewards = None\n        self.best_reward = -np.inf\n        self.save_path = save_path\n        self.reward_threshold = reward_threshold\n        os.makedirs(self.save_path, exist_ok=True)\n\n    def _on_training_start(self) -> None:\n        self.current_rewards = [0.0] * self.training_env.num_envs\n        \n    def _on_step(self) -> bool:\n        rewards = self.locals[\"rewards\"]\n        dones = self.locals[\"dones\"]\n\n        for i in range(len(rewards)):\n            self.current_rewards[i] += rewards[i]\n            if dones[i]:\n                total = self.current_rewards[i]\n                episode_rewards2.append(total)\n                print(f\"Episodio completado: reward = {total:.2f}\")\n\n                # Guardar modelo si reward supera el umbral y es el mejor\n                if total >= self.reward_threshold and total > self.best_reward:\n                    self.best_reward = total\n                    filename = f\"{self.save_path}/best_model_ep{self.num_timesteps}_rew{total:.1f}\".replace(\".\", \"_\")\n                    self.model.save(filename)\n                    print(f\"💾 Guardado nuevo mejor modelo con reward = {total:.2f} en {filename}.zip\")\n\n                self.current_rewards[i] = 0.0\n\n        # Registrar pérdida (entropía del actor)\n        maybe_loss = self.model.logger.name_to_value.get(\"train/entropy_loss\")\n        if maybe_loss is not None:\n            episode_losses.append(maybe_loss)\n\n        return True\n\n    def _on_rollout_end(self) -> None:\n        if \"rollout/ep_rew_mean\" in self.model.logger.name_to_value:\n            rew = self.model.logger.name_to_value[\"rollout/ep_rew_mean\"]\n            episode_rewards.append(rew)\n            print(f\"Recompensa en rollout completado: {rew:.2f}\")\n\n\n# ─── CALLBACK DE CHECKPOINTS CADA 500K PASOS ───────────────────────────────────\ncheckpoint_callback = CheckpointCallback(\n    save_freq=500_000,\n    save_path=\"./checkpoints\",\n    name_prefix=\"ppo_doom_phase1\"\n)\n\n# ─── DEFINICIÓN DEL MODELO PPO CON CNN PERSONALIZADA ──────────────────────────\npolicy_kwargs = dict(\n    features_extractor_class=CustomCNN,\n    features_extractor_kwargs=dict(features_dim=512)\n)\n\nmodel = PPO(\n    policy=\"CnnPolicy\",\n    env=env_train,\n    learning_rate=1e-4,\n    n_steps=1024,\n    batch_size=1024,\n    n_epochs=4,\n    ent_coef=0.02,  # ← Aumenta la entropía para explorar más\n    policy_kwargs=policy_kwargs,\n    verbose=1,\n    device=device\n)\n\nlogging_callback = LoggingCallback(save_path=\"best_models_inline\", reward_threshold=50.0)\n\nmodel.learn(\n    total_timesteps=9_000_000,\n    callback=[checkpoint_callback, logging_callback]\n)\n\n\n# ─── GUARDAR MODELO ENTRENADO ─────────────────────────────────────────────────\nmodel.save(\"ppo_doom_bots_phase1\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\n# ─── GRÁFICO DE REWARD POR EPISODIO ────────────────────────────────────────────\nplt.subplot(1, 2, 1)\nplt.plot(episode_rewards)\nplt.title(\"Recompensa por rollout (no promedio móvil)\")\nplt.xlabel(\"Rollout (cada n_steps pasos)\")\nplt.ylabel(\"Recompensa\")\nplt.grid(True)\n\n# ─── GRÁFICO DE ENTROPÍA (PROXY DE PÉRDIDA) ───────────────────────────────────\nplt.subplot(1, 2, 2)\nplt.plot(episode_losses)\nplt.title(\"Entropía del actor (proxy de pérdida)\")\nplt.xlabel(\"Paso de entrenamiento\")\nplt.ylabel(\"Entropía\")\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\nplt.figure(figsize=(10, 4))\nplt.plot(episode_rewards2)\nplt.title(\"Recompensa por episodio\")\nplt.xlabel(\"Episodio\")\nplt.ylabel(\"Reward total\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_and_record(model, filename=\"doom_phase1_eval.mp4\", num_episodes=5):\n    from IPython.display import Video, display\n    import time\n\n    env_eval = DummyVecEnv([make_env()])\n    env_eval = VecFrameStack(env_eval, n_stack=4, channels_order='first')\n    env_eval.venv.render_mode = \"rgb_array\"\n    \n    writer = imageio.get_writer(filename, fps=15)  # Puedes ajustar el fps aquí\n    rewards = []\n\n    for ep in range(num_episodes):\n        obs = env_eval.reset()\n        done = [False]\n        total_reward = 0.0\n\n        while not done[0]:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, _ = env_eval.step(action)\n            total_reward += reward[0]\n\n            # Render del entorno\n            frame = env_eval.render(mode='rgb_array')\n            if frame is not None:\n                writer.append_data(frame)\n\n        print(f\"Episodio {ep+1}: reward = {total_reward:.2f}\")\n        rewards.append(total_reward)\n\n    writer.close()\n    print(\"Recompensas por episodio:\", rewards)\n    display(Video(filename, embed=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_and_record(model, filename=\"doom_phase1_eval.mp4\", num_episodes=5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}