{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/oscarandresgutierrez/dqn-doom-v0?scriptVersionId=252095619\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install vizdoom==1.2.4 gymnasium pyvirtualdisplay imageio imageio-ffmpeg \\\n        opencv-python-headless torch torchvision --quiet\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:00:28.55677Z","iopub.execute_input":"2025-07-22T17:00:28.557241Z","iopub.status.idle":"2025-07-22T17:00:32.560444Z","shell.execute_reply.started":"2025-07-22T17:00:28.557206Z","shell.execute_reply":"2025-07-22T17:00:32.559038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyvirtualdisplay import Display; Display(visible=0, size=(640,480)).start()\n\nimport gymnasium as gnm, numpy as np, random, collections, time, cv2, imageio, torch\nimport torch.nn as nn, torch.nn.functional as F\nfrom vizdoom import gymnasium_wrapper          # registra los entornos VizDoom\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:05:54.481632Z","iopub.execute_input":"2025-07-22T17:05:54.481964Z","iopub.status.idle":"2025-07-22T17:05:54.567823Z","shell.execute_reply.started":"2025-07-22T17:05:54.481942Z","shell.execute_reply":"2025-07-22T17:05:54.566794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import vizdoom as vzd\nimport gymnasium as gnm\nimport cv2, numpy as np\n\n# ── 84×84 escala de grises  (sin canal extra) ─────────────────────\nclass GrayResize(gnm.ObservationWrapper):\n    \"\"\"\n    Convierte obs[\"screen\"] a escala de grises, la redimensiona a 84×84\n    y devuelve un ndarray uint8 de shape (84,84)  (sin eje de canal).\n    \"\"\"\n    def __init__(self, env, shape=(84, 84)):\n        super().__init__(env)\n        self.shape = shape\n        self.observation_space = gnm.spaces.Box(\n            0, 255, shape, dtype=np.uint8\n        )\n\n    def observation(self, obs):\n        frame = cv2.cvtColor(obs[\"screen\"], cv2.COLOR_RGB2GRAY)\n        return cv2.resize(frame, self.shape, interpolation=cv2.INTER_AREA)\n\n# ── reward‑shaping (daño + disparo fallido) ───────────────────────\nclass RewardShaper(gnm.Wrapper):\n    def __init__(self, env, k_hp=-0.1, k_shot=-0.05):\n        super().__init__(env)\n        self.k_hp, self.k_shot = k_hp, k_shot\n        self.game = self.env.unwrapped.game\n        btns = list(self.game.get_available_buttons())\n        self.attack_idx = btns.index(vzd.Button.ATTACK)\n        self.hp_prev = self.frag_prev = None\n\n    def reset(self, **kw):\n        obs, info = self.env.reset(**kw)\n        self.hp_prev   = self.game.get_game_variable(vzd.GameVariable.HEALTH)\n        self.frag_prev = self.game.get_game_variable(vzd.GameVariable.FRAGCOUNT)\n        return obs, info\n\n    def step(self, act_idx):\n        obs, r, term, trunc, info = self.env.step(act_idx)\n        hp   = self.game.get_game_variable(vzd.GameVariable.HEALTH)\n        frag = self.game.get_game_variable(vzd.GameVariable.FRAGCOUNT)\n\n        dmg   = max(self.hp_prev - hp, 0)\n        shot  = bool(self.game.get_last_action()[self.attack_idx])\n        miss_pen = self.k_shot if shot and frag == self.frag_prev else 0\n\n        shaped = r + self.k_hp * dmg + miss_pen\n        self.hp_prev, self.frag_prev = hp, frag\n        return obs, shaped, term, trunc, info\n\n# ── fábrica de entornos ────────────────────────────────────────────\ndef make_env(seed=None):\n    env = gnm.make(\"VizdoomDefendLine-v0\", render_mode=\"rgb_array\", frame_skip=4)\n    env = GrayResize(env)\n    env = gnm.wrappers.FrameStack(env, 4)        # (84,84,4)\n    env = RewardShaper(env)\n    env.action_space.seed(seed)\n    return env\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:23:20.835899Z","iopub.execute_input":"2025-07-22T17:23:20.836961Z","iopub.status.idle":"2025-07-22T17:23:20.852959Z","shell.execute_reply.started":"2025-07-22T17:23:20.836923Z","shell.execute_reply":"2025-07-22T17:23:20.851936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, cap, obs_shape):\n        self.cap = cap\n        self.ptr = 0; self.full = False\n        self.s  = np.empty((cap, *obs_shape), np.uint8)\n        self.a  = np.empty((cap,),            np.int64)\n        self.r  = np.empty((cap,),            np.float32)\n        self.s2 = np.empty((cap, *obs_shape), np.uint8)\n        self.d  = np.empty((cap,),            np.bool_)\n    def add(self, s,a,r,s2,d):\n        self.s[self.ptr], self.a[self.ptr], self.r[self.ptr] = s, a, r\n        self.s2[self.ptr], self.d[self.ptr] = s2, d\n        self.ptr = (self.ptr + 1) % self.cap\n        self.full |= self.ptr == 0\n    def sample(self, batch):\n        idx = np.random.randint(0, self.cap if self.full else self.ptr, batch)\n        to_t = lambda x, dtype=None: torch.as_tensor(x, device=device, dtype=dtype)\n        return (to_t(self.s[idx]),\n                to_t(self.a[idx]),\n                to_t(self.r[idx]),\n                to_t(self.s2[idx]),\n                to_t(self.d[idx]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:05:55.152176Z","iopub.execute_input":"2025-07-22T17:05:55.152539Z","iopub.status.idle":"2025-07-22T17:05:55.16061Z","shell.execute_reply.started":"2025-07-22T17:05:55.152514Z","shell.execute_reply":"2025-07-22T17:05:55.159771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoomDQN(nn.Module):\n    def __init__(self, n_actions):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(4,32,8,4), nn.ReLU(),\n            nn.Conv2d(32,64,4,2), nn.ReLU(),\n            nn.Conv2d(64,64,3,1), nn.ReLU())\n        self.head = nn.Sequential(\n            nn.Linear(7*7*64,512), nn.ReLU(),\n            nn.Linear(512,n_actions))\n    def forward(self, x):\n        x = x.float() / 255.0\n        x = self.conv(x).view(x.size(0), -1)\n        return self.head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:05:55.553179Z","iopub.execute_input":"2025-07-22T17:05:55.553568Z","iopub.status.idle":"2025-07-22T17:05:55.559838Z","shell.execute_reply.started":"2025-07-22T17:05:55.553523Z","shell.execute_reply":"2025-07-22T17:05:55.559064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ── helpers ──────────────────────────────────────────────────────\n# ── helper: garantiza (C,84,84) ───────────────────────────────────\ndef to_chw(obs):\n    \"\"\"\n    Convierte cualquier arreglo con un eje‑canal de tamaño 4\n    a formato (4,84,84) uint8.\n\n    Maneja:\n      • (84,84,4)   → (4,84,84)\n      • (84,4,84)   → (4,84,84)\n      • (4,84,84)   → (4,84,84)  (ya correcto)\n    \"\"\"\n    arr = np.array(obs, copy=False).squeeze()\n\n    if arr.ndim == 3:\n        # si ya está en CHW, shape[0] == 4 → nada que hacer\n        if arr.shape[0] != 4:\n            # localiza eje cuyo tamaño es 4 y lo mueve a la posición 0\n            ch_axis = int(np.where(np.array(arr.shape) == 4)[0][0])\n            arr = np.moveaxis(arr, ch_axis, 0)\n\n    return arr\n\ndef record_checkpoint(net, step, max_frames=1_000, fps=30):\n    \"\"\"\n    • Guarda los pesos en  «dqn_step_<step>.pth»\n    • Graba un episodio determinista en «dqn_step_<step>.mp4»\n    \"\"\"\n    tag = f\"dqn_step_{step:,}\"\n    torch.save(net.state_dict(), f\"{tag}.pth\")\n\n    env_eval = make_env()\n    frames, ep_R = [], 0\n    s,_ = env_eval.reset(); s = to_chw(s); done = False\n    with torch.no_grad():\n        while not done and len(frames) < max_frames:\n            a = net(torch.tensor(s[None], device=device)).argmax(1).item()\n            nxt, r, done, term, _ = env_eval.step(a)\n            frames.append(env_eval.render())\n            s, ep_R = to_chw(nxt), ep_R + r\n    env_eval.close()\n    imageio.mimsave(f\"{tag}.mp4\", frames, fps=fps)\n    print(f\"💾 guardado {tag}.pth   🎞️ {tag}.mp4   |  Reward={ep_R:.1f}\")\n\n# ── hiperparámetros ─────────────────────────────────────────────\nMAX_STEPS      = 5_000_000          # 🔧 pasos totales\nLEARN_START    = 10_000\nBATCH          = 32\nGAMMA          = 0.99\nLR             = 1e-4\nTARGET_SYNC    = 10_000\nEPS_START, EPS_END, EPS_DECAY = 1.0, 0.05, 1_000_000\nBUFFER_SIZE    = 100_000\nSEED           = 42\n\n# ── inicialización ──────────────────────────────────────────────\nenv       = make_env(SEED)\nn_actions = env.action_space.n\nq_net     = DoomDQN(n_actions).to(device)\ntgt_net   = DoomDQN(n_actions).to(device); tgt_net.load_state_dict(q_net.state_dict())\nopt       = torch.optim.Adam(q_net.parameters(), lr=LR)\nbuf       = ReplayBuffer(BUFFER_SIZE, (4,84,84))\n\nrewards, eps = [], EPS_START\nstate,_ = env.reset(seed=SEED); state = to_chw(state)\nep_R, t0 = 0, time.time()\n\n# ── bucle de entrenamiento ──────────────────────────────────────\nfor step in range(1, MAX_STEPS + 1):\n\n    # ε‑greedy\n    if random.random() < eps:\n        action = env.action_space.sample()\n    else:\n        with torch.no_grad():\n            action = int(q_net(torch.tensor(state[None], device=device)).argmax(1))\n\n    # transición\n    nxt, r, done, term, _ = env.step(action)\n    nxt = to_chw(nxt)\n    buf.add(state, action, r, nxt, done or term)\n    state, ep_R = nxt, ep_R + r\n\n    # aprendizaje\n    if step > LEARN_START:\n        s, a, rn, s2, d = buf.sample(BATCH)\n        q_pred = q_net(s).gather(1, a.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            a2 = q_net(s2).argmax(1)\n            q_next = tgt_net(s2).gather(1, a2.unsqueeze(1)).squeeze(1)\n            y = rn + GAMMA * q_next * (~d)\n        loss = F.smooth_l1_loss(q_pred, y)\n\n        opt.zero_grad(); loss.backward()\n        nn.utils.clip_grad_norm_(q_net.parameters(), 10)\n        opt.step()\n\n        if step % TARGET_SYNC == 0:\n            tgt_net.load_state_dict(q_net.state_dict())\n        eps = max(EPS_END, EPS_START - (step - LEARN_START) / EPS_DECAY)\n\n    # fin de episodio\n    if done or term:\n        rewards.append(ep_R)\n        ep_R = 0\n        state,_ = env.reset(); state = to_chw(state)\n\n    # logging\n    if step % 10000 == 0:\n        avg = np.mean(rewards[-100:]) if rewards else 0\n        print(f\"Paso {step:,} | ε={eps:.3f} | Reward_100ep={avg:6.2f}\")\n\n    # 🔖 checkpoint + vídeo cada 50 k pasos\n    if step % 500_000 == 0:\n        record_checkpoint(q_net, step)\n\nenv.close()\ntorch.save(q_net.state_dict(), \"dqn_defendline_final.pth\")\nprint(f\"\\nEntrenamiento finalizado en {(time.time()-t0)/60:.1f} min — modelo guardado.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:42:34.702741Z","iopub.execute_input":"2025-07-22T17:42:34.703095Z","iopub.status.idle":"2025-07-22T17:42:57.280726Z","shell.execute_reply.started":"2025-07-22T17:42:34.703066Z","shell.execute_reply":"2025-07-22T17:42:57.278924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom IPython.display import Video, display\n\ndef play_and_record(model_path, vid=\"dqn_defendline_eval.mp4\", episodes=3):\n    env = make_env()\n    net = DoomDQN(env.action_space.n).to(device)\n    net.load_state_dict(torch.load(model_path, map_location=device))\n    net.eval()\n\n    frames, ep_ret = [], []\n    for _ in range(episodes):\n        s,_ = env.reset(); s = to_chw(s); R, done = 0, False\n        while not done:\n            with torch.no_grad():\n                a = net(torch.tensor(s[None], device=device)).argmax(1).item()\n            s2,r,done,_,_ = env.step(a)\n            frames.append(env.render())        # RGB frame\n            s, R = to_chw(s2), R + r\n        ep_ret.append(R)\n    env.close()\n    imageio.mimsave(vid, frames, fps=30)\n    return vid, ep_ret\n\nvid_path, eval_R = play_and_record(\"dqn_defendline.pth\")\ndisplay(Video(vid_path, embed=True, height=480))\n\nplt.figure(figsize=(6,3))\nplt.plot(rewards, label=\"Train reward (ep.)\")\nplt.axhline(np.mean(eval_R), color=\"r\", ls=\"--\", label=f\"Eval avg ({np.mean(eval_R):.1f})\")\nplt.xlabel(\"Episodio\"); plt.ylabel(\"Reward\"); plt.title(\"DQN – Vizdoom DefendLine\")\nplt.legend(); plt.tight_layout(); plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T17:26:51.608268Z","iopub.status.idle":"2025-07-22T17:26:51.608577Z","shell.execute_reply.started":"2025-07-22T17:26:51.608437Z","shell.execute_reply":"2025-07-22T17:26:51.60845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}