{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Entrenamiento de un Agente en el Entorno `Pendulum-v1` con TF-Agents\n\n## ¬øQu√© es Gymnasium?\n\n**Gymnasium** es una biblioteca de Python desarrollada por la comunidad de aprendizaje por refuerzo, dise√±ada para proporcionar una interfaz estandarizada para una amplia variedad de entornos simulados. Es el sucesor directo de `OpenAI Gym`, ahora mantenido por la organizaci√≥n Farama. \n\nPermite a los investigadores y desarrolladores:\n- Probar algoritmos de aprendizaje por refuerzo (RL) de forma reproducible.\n- Comparar distintos enfoques usando entornos bien definidos.\n- Acceder a entornos cl√°sicos, continuos, discretos y visuales, desde juegos simples hasta simulaciones complejas.\n\n---\n\n## ¬øQu√© es el entorno `Pendulum-v1`?\n\nEl entorno `Pendulum-v1` es un entorno cl√°sico de control continuo. Su objetivo es controlar un p√©ndulo invertido, es decir, aplicar torques para mantenerlo erguido en posici√≥n vertical, sin que caiga por gravedad.\n\nEste entorno es un excelente caso de estudio para agentes que deben aprender pol√≠ticas de control en **espacios de acci√≥n continuos**.\n\n---\n\n## Detalles del entorno\n\n### Observaci√≥n (`observation`)\n\nEl espacio de observaci√≥n es un vector de **dimensi√≥n 3**, que representa el estado del p√©ndulo:\n\n- `cos(Œ∏)` : el coseno del √°ngulo del p√©ndulo respecto a la vertical.\n- `sin(Œ∏)` : el seno del √°ngulo.\n- `Œ∏_dot` : la velocidad angular del p√©ndulo.\n\n> La observaci√≥n codifica el √°ngulo usando `cos(Œ∏)` y `sin(Œ∏)` para evitar discontinuidades en ¬±œÄ.\n\n---\n\n### Acci√≥n (`action`)\n\nEl espacio de acci√≥n es **continuo**, con una √∫nica dimensi√≥n:\n\n- Acci√≥n ‚àà **[-2, 2]**, que representa el torque aplicado al p√©ndulo.\n\n> Este entorno **no permite acciones discretas**, lo cual tiene implicaciones en el tipo de algoritmos que se pueden usar para resolverlo (ver m√°s abajo).\n\n---\n\n### Recompensa (`reward`)\n\nLa recompensa se calcula en cada paso como:\n\n\\[\nr = - (\\theta^2 + 0.1 \\cdot \\dot{\\theta}^2 + 0.001 \\cdot a^2)\n\\]\n\nDonde:\n- `Œ∏` es el √°ngulo respecto a la posici√≥n vertical (idealmente cero),\n- `Œ∏_dot` es la velocidad angular,\n- `a` es el torque aplicado.\n\nEsto penaliza:\n- El alejamiento del √°ngulo deseado (`Œ∏ ‚â† 0`),\n- La velocidad angular alta,\n- El uso excesivo de energ√≠a (grandes torques).\n\n> El agente debe aprender a estabilizar el p√©ndulo en posici√≥n vertical con el m√≠nimo esfuerzo posible.\n\n---\n\n## ¬øQu√© tipo de redes son apropiadas?\n\nDado que el entorno tiene un **espacio de acci√≥n continuo**, **no se pueden utilizar algoritmos basados en Q-Learning discreto**, como:\n\n- `DQN` (Deep Q-Network)\n- `Double DQN`\n- `Dueling DQN`\n\nEstos algoritmos est√°n dise√±ados para problemas con un conjunto finito de acciones.\n\n---\n\n### Algoritmos apropiados (actor-cr√≠tico continuo)\n\nPara `Pendulum-v1`, se deben usar **algoritmos actor-cr√≠tico** capaces de manejar acciones continuas:\n\n- **DDPG** (Deep Deterministic Policy Gradient)\n- **TD3** (Twin Delayed DDPG)\n- **SAC** (Soft Actor-Critic)\n- **PPO** (Proximal Policy Optimization, si se adapta)\n\nEstos algoritmos utilizan dos redes principales:\n\n1. **Actor (Pol√≠tica):** aprende a generar acciones √≥ptimas.\n2. **Cr√≠tico (Q-Value):** estima el valor de las acciones tomadas.\n\nEste cuaderno implementar√° un agente basado en **DDPG**, entrenado con la biblioteca **TF-Agents**, y al final comparar√° su desempe√±o frente a una pol√≠tica aleatoria.\n\n---\n\n## ¬øQu√© analizaremos?\n\n- Entrenamiento del agente con DDPG en `Pendulum-v1`.\n- Gr√°fica del reward acumulado a lo largo del entrenamiento.\n- Video del agente entrenado.\n- Video de un agente que act√∫a aleatoriamente, como comparaci√≥n.\n\n---\n\n","metadata":{}},{"cell_type":"code","source":"# ‚úÖ Instalar dependencias necesarias\n!sudo apt-get update\n!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n!pip install 'imageio==2.4.0'\n!pip install pyvirtualdisplay\n!pip install tf-agents[reverb]\n!pip install pyglet\n!pip install gymnasium[classic-control]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T19:59:57.521122Z","iopub.execute_input":"2025-06-25T19:59:57.521789Z","iopub.status.idle":"2025-06-25T20:03:19.438571Z","shell.execute_reply.started":"2025-06-25T19:59:57.521748Z","shell.execute_reply":"2025-06-25T20:03:19.436358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"xvfb: Sistema de framebuffer virtual que permite renderizar interfaces gr√°ficas sin un monitor f√≠sico (√∫til en Colab o servidores remotos).\n\nffmpeg: Herramienta para codificar, grabar y convertir videos. Se usa para guardar los videos de simulaci√≥n del agente.\n\nfreeglut3-dev: Implementaci√≥n de OpenGL utilizada por los entornos visuales como Pendulum o CartPole.\n\nimageio==2.4.0: Librer√≠a para crear y guardar im√°genes/videos. La versi√≥n 2.4.0 es compatible con TF-Agents y pyvirtualdisplay.\n\npyvirtualdisplay: Permite emular una pantalla virtual, esencial para renderizar video en entornos sin GUI.\n\ntf-agents[reverb]: Librer√≠a de TensorFlow para aprendizaje por refuerzo. La opci√≥n [reverb] activa un buffer de repetici√≥n eficiente para almacenar transiciones.\n\npyglet: Librer√≠a gr√°fica para Python, usada por muchos entornos de gym para mostrar animaciones.\n\ngymnasium[classic-control]: Versi√≥n moderna de gym con soporte para entornos cl√°sicos como Pendulum-v1, CartPole-v1, entre otros.","metadata":{}},{"cell_type":"code","source":"# === Imports principales ===\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tempfile\nimport os\n\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.environments import gym_wrapper\nfrom tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\nfrom tf_agents.networks.value_network import ValueNetwork\nfrom tf_agents.agents.ddpg import ddpg_agent\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.drivers import dynamic_step_driver\n\nimport imageio\nfrom pyvirtualdisplay import Display\nimport gym\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:19.442676Z","iopub.execute_input":"2025-06-25T20:03:19.443302Z","iopub.status.idle":"2025-06-25T20:03:34.242279Z","shell.execute_reply.started":"2025-06-25T20:03:19.443240Z","shell.execute_reply":"2025-06-25T20:03:34.240940Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"numpy, tensorflow, matplotlib: Librer√≠as fundamentales para manejo de arrays, entrenamiento de redes neuronales y visualizaci√≥n.\n\ntempfile, os: Herramientas del sistema para manejo de archivos temporales y operaciones sobre el sistema de archivos.\n\nTF-Agents (tf_agents.*):\n\nenvironments, gym_wrapper: Permiten envolver entornos de gym para ser compatibles con TF-Agents.\n\nnetworks: Redes neuronales base para actores y cr√≠ticos en DDPG.\n\nddpg_agent: Implementaci√≥n del agente DDPG (Deep Deterministic Policy Gradient).\n\ntf_uniform_replay_buffer: Buffer de repetici√≥n para almacenar experiencias de entrenamiento.\n\ntrajectory: Estructura que define las transiciones (estado, acci√≥n, recompensa, etc).\n\ncommon: Funciones utilitarias para entrenamiento (ej. optimizadores, inicializaci√≥n).\n\nrandom_tf_policy: Pol√≠tica aleatoria √∫til para inicializaci√≥n del buffer.\n\ntf_metrics, dynamic_step_driver: M√©tricas de desempe√±o y colectores de pasos.\n\nimageio: Permite crear y guardar videos cuadro por cuadro.\n\npyvirtualdisplay: Crea un display virtual (necesario para renderizar videos en Colab).\n\ngym: Librer√≠a base de entornos de control cl√°sico y otros benchmarks.","metadata":{}},{"cell_type":"code","source":"# === Activar display virtual para renderizar video\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()\n\n# === Crear entornos usando gym cl√°sico\nenv_name = \"Pendulum-v1\"\ntrain_py_env = gym.make(env_name)\neval_py_env = gym.make(env_name)\n\n# ‚úÖ Envolverlos con GymWrapper\ntrain_py_env = gym_wrapper.GymWrapper(train_py_env)\neval_py_env = gym_wrapper.GymWrapper(eval_py_env)\n\n# ‚úÖ Convertir a TFPyEnvironment\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# === Verificar specs\nprint(\"üîç Observaci√≥n:\", train_env.observation_spec())\nprint(\"üéÆ Acci√≥n:\", train_env.action_spec())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:34.244233Z","iopub.execute_input":"2025-06-25T20:03:34.245056Z","iopub.status.idle":"2025-06-25T20:03:35.485328Z","shell.execute_reply.started":"2025-06-25T20:03:34.245011Z","shell.execute_reply":"2025-06-25T20:03:35.483416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Se crea un display virtual con pyvirtualdisplay. Esto es necesario en Kaggle y otros entornos sin GUI para poder renderizar los entornos de Gym y grabar videos (rgb_array).\n\nSe selecciona el entorno cl√°sico Pendulum-v1 de Gym.\nSe crean dos instancias: una para entrenamiento (train_py_env) y otra para evaluaci√≥n (eval_py_env).\n\nGymWrapper convierte los entornos de gym en objetos compatibles con TF-Agents.\n\nTFPyEnvironment convierte los entornos envueltos en estructuras completamente integradas con TensorFlow (permite ejecuci√≥n en grafos, entrenamiento eficiente, etc.).\n\nSe imprimen las especificaciones de observaci√≥n y acci√≥n, que describen el espacio de entradas y salidas del entorno, √∫til para definir redes compatibles con el agente.","metadata":{}},{"cell_type":"code","source":"# === Extraer specs ===\nobservation_spec = train_env.observation_spec()\naction_spec = train_env.action_spec()\ntime_step_spec = train_env.time_step_spec()\n\n# === Redes personalizadas compatibles con TF-Agents ===\nfrom tf_agents.networks import network","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:35.489815Z","iopub.execute_input":"2025-06-25T20:03:35.490236Z","iopub.status.idle":"2025-06-25T20:03:35.501760Z","shell.execute_reply.started":"2025-06-25T20:03:35.490208Z","shell.execute_reply":"2025-06-25T20:03:35.498433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Se extraen las especificaciones del entorno para:\n\nobservation_spec: define la forma y tipo de datos de las observaciones (input).\n\naction_spec: define el rango y tipo de las acciones posibles (output del actor).\n\ntime_step_spec: describe la estructura completa de un paso del entorno (incluye observaci√≥n, recompensa, estado terminal, etc.).\n\nEstas especificaciones son necesarias para construir redes compatibles con el agente.\n\nSe importa la clase base network.Network, desde la cual se derivan las redes personalizadas de actor y cr√≠tico. Esta clase define una estructura com√∫n para que TF-Agents gestione redes en sus agentes.\n\n","metadata":{}},{"cell_type":"markdown","source":"En este proyecto usamos el algoritmo Deep Deterministic Policy Gradient (DDPG), que requiere dos redes principales:\n\nüü¢ Actor ‚Äì genera la acci√≥n continua que el agente tomar√° dada una observaci√≥n.\n\nüîµ Cr√≠tico (Critic) ‚Äì estima el valor Q de una pareja (observaci√≥n, acci√≥n).\n\n","metadata":{}},{"cell_type":"code","source":"class CustomActor(network.Network):\n    def __init__(self, observation_spec, action_spec, name='CustomActor'):\n        super().__init__(input_tensor_spec=observation_spec, state_spec=(), name=name)\n        self._action_limit = tf.constant(action_spec.maximum, dtype=tf.float32)\n        self.model = tf.keras.Sequential([\n            tf.keras.layers.InputLayer(input_shape=observation_spec.shape),\n            tf.keras.layers.Dense(128, activation='relu'),\n            tf.keras.layers.Dense(128, activation='relu'),\n            tf.keras.layers.Dense(action_spec.shape[0], activation='tanh'),\n            tf.keras.layers.Lambda(lambda x: x * self._action_limit)\n        ])\n\n    def call(self, obs, step_type=None, network_state=(), training=False):\n        return self.model(obs), network_state\n\nclass CustomCritic(network.Network):\n    def __init__(self, observation_spec, action_spec, name='CustomCritic'):\n        input_spec = (observation_spec, action_spec)\n        super().__init__(input_tensor_spec=input_spec, state_spec=(), name=name)\n        self.concat = tf.keras.layers.Concatenate()\n        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n        self.q_out = tf.keras.layers.Dense(1)\n\n    def call(self, inputs, step_type=None, network_state=(), training=False):\n        obs, act = inputs\n        x = self.concat([obs, act])\n        x = self.fc1(x)\n        x = self.fc2(x)\n        q = self.q_out(x)\n    \n        # üîß Asegurar que q tiene forma [B, 1]\n        q = tf.squeeze(q, axis=-1)  # ‚úÖ Corregido: salida 1D\n        return q, network_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:35.503951Z","iopub.execute_input":"2025-06-25T20:03:35.504471Z","iopub.status.idle":"2025-06-25T20:03:35.627446Z","shell.execute_reply.started":"2025-06-25T20:03:35.504427Z","shell.execute_reply":"2025-06-25T20:03:35.626171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Capa Actor\n\n‚úîÔ∏è Objetivo\nAprender una funci√≥n determinista Œº(s) ‚âà a, es decir, dada una observaci√≥n del entorno s, predecir la acci√≥n continua a que maximice la recompensa a largo plazo.\n\n‚úîÔ∏è Estructura\nCapa\tDescripci√≥n\nInputLayer\tRecibe vectores de observaci√≥n (estado del entorno).\nDense(128, relu)\tPrimera capa oculta, aprende representaciones no lineales.\nDense(128, relu)\tSegunda capa oculta, mayor capacidad de aprendizaje.\nDense(tanh)\tCapa de salida: valores entre -1 y 1, que luego se escalan.\nLambda(x * max_a)\tEscala la salida seg√∫n el rango permitido por el entorno (action_spec).\n\n‚úîÔ∏è Justificaci√≥n\nSe usan dos capas densas con ReLU por su buena capacidad de aproximaci√≥n no lineal.\n\nLa salida se pasa por una funci√≥n tanh, que limita el rango a [-1, 1].\n\nLuego se escala al l√≠mite de acci√≥n permitido por el entorno, lo que garantiza que las acciones propuestas sean v√°lidas.\n\n#Capa Cr√≠tica\n\n‚úîÔ∏è Objetivo\nAprender una funci√≥n determinista Œº(s) ‚âà a, es decir, dada una observaci√≥n del entorno s, predecir la acci√≥n continua a que maximice la recompensa a largo plazo.\n\n‚úîÔ∏è Estructura\nCapa\tDescripci√≥n\nInputLayer\tRecibe vectores de observaci√≥n (estado del entorno).\nDense(128, relu)\tPrimera capa oculta, aprende representaciones no lineales.\nDense(128, relu)\tSegunda capa oculta, mayor capacidad de aprendizaje.\nDense(tanh)\tCapa de salida: valores entre -1 y 1, que luego se escalan.\nLambda(x * max_a)\tEscala la salida seg√∫n el rango permitido por el entorno (action_spec).\n\n‚úîÔ∏è Justificaci√≥n\nSe usan dos capas densas con ReLU por su buena capacidad de aproximaci√≥n no lineal.\n\nLa salida se pasa por una funci√≥n tanh, que limita el rango a [-1, 1].\n\nLuego se escala al l√≠mite de acci√≥n permitido por el entorno, lo que garantiza que las acciones propuestas sean v√°lidas.","metadata":{}},{"cell_type":"markdown","source":"Aunque TF-Agents ofrece redes predefinidas como ActorDistributionNetwork y QNetwork, optamos por definir nuestras propias redes para:\n\nTener control total sobre la arquitectura, activaciones y escalado.\n\nAjustar el output del actor al espacio de acciones continuas espec√≠ficas del entorno (con tanh + escalado).\n\nAdaptar f√°cilmente la forma de salida del cr√≠tico (q) para que sea compatible con el entrenamiento (tf.squeeze).","metadata":{}},{"cell_type":"code","source":"# === Crear instancias de las redes\nactor_model = CustomActor(observation_spec, action_spec)\ncritic_model = CustomCritic(observation_spec, action_spec)\n\n# ‚úÖ Inicializar variables\ndummy_obs = tf.random.uniform((1,) + observation_spec.shape)\ndummy_act = tf.random.uniform((1,) + action_spec.shape)\nactor_model(dummy_obs)\ncritic_model((dummy_obs, dummy_act))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:35.629137Z","iopub.execute_input":"2025-06-25T20:03:35.629454Z","iopub.status.idle":"2025-06-25T20:03:35.946590Z","shell.execute_reply.started":"2025-06-25T20:03:35.629431Z","shell.execute_reply":"2025-06-25T20:03:35.944930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Despu√©s de definir las arquitecturas CustomActor y CustomCritic, creamos sus instancias concretas y forzamos una inicializaci√≥n expl√≠cita de sus variables, que es un paso necesario antes de usarlas dentro del agente DDPG.\n\nAqu√≠ construimos:\n\nactor_model: red neuronal que toma observaciones del entorno y devuelve una acci√≥n continua v√°lida.\n\ncritic_model: red que toma un par (estado, acci√≥n) y devuelve el valor esperado (Q).\n\nEstas redes personalizadas siguen las interfaces esperadas por TF-Agents, por lo que son totalmente compatibles con el agente DdpgAgent.\n\n\nSe generan tensores aleatorios dummy_obs y dummy_act que respetan las dimensiones del entorno.\n\nEstas muestras se pasan a las redes una vez para que TensorFlow cree internamente sus variables y estructuras.\n\nEsto es especialmente √∫til cuando se usan redes personalizadas con capas como Dense, que requieren inicializar pesos antes de ser entrenadas.\n\n‚ö†Ô∏è Si este paso se omite, podr√≠as recibir errores durante la inicializaci√≥n del agente DDPG, ya que internamente accede a los pesos del modelo.","metadata":{}},{"cell_type":"code","source":"# === Crear el agente\nagent = ddpg_agent.DdpgAgent(\n    time_step_spec=time_step_spec,\n    action_spec=action_spec,\n    actor_network=actor_model,\n    critic_network=critic_model,\n    actor_optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    critic_optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    ou_stddev=0.2,\n    ou_damping=0.15,\n    target_update_tau=0.005,\n    target_update_period=5,\n    gamma=0.99,\n    reward_scale_factor=1.0,\n    train_step_counter=tf.Variable(0)\n)\n\nagent.initialize()\nprint(\"‚úÖ Agente DDPG inicializado con √©xito\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:35.948263Z","iopub.execute_input":"2025-06-25T20:03:35.948776Z","iopub.status.idle":"2025-06-25T20:03:36.153544Z","shell.execute_reply.started":"2025-06-25T20:03:35.948734Z","shell.execute_reply":"2025-06-25T20:03:36.152051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"El agente DDPG (Deep Deterministic Policy Gradient) es un algoritmo de aprendizaje por refuerzo off-policy dise√±ado para espacios de acci√≥n continuos. Se basa en el enfoque actor-cr√≠tico, donde:\n\nEl actor aprende una pol√≠tica determinista.\n\nEl cr√≠tico estima el valor Q de cada par (estado, acci√≥n).\n\n‚öôÔ∏è Par√°metros claves\n| Par√°metro                 | Descripci√≥n                                                            |\n| ------------------------- | ---------------------------------------------------------------------- |\n| `time_step_spec`          | Especificaci√≥n de entradas del entorno (`observation`, `reward`, etc.) |\n| `action_spec`             | Especificaci√≥n del espacio de acci√≥n continua del entorno              |\n| `actor_network`           | Red neuronal personalizada que genera acciones a partir del estado     |\n| `critic_network`          | Red neuronal que estima el valor Q para un par (estado, acci√≥n)        |\n| `actor_optimizer`         | Optimizador del actor (m√°s lento, `1e-4`)                              |\n| `critic_optimizer`        | Optimizador del cr√≠tico (m√°s r√°pido, `1e-3`)                           |\n| `ou_stddev`, `ou_damping` | Par√°metros del proceso Ornstein-Uhlenbeck para exploraci√≥n ruidosa     |\n| `target_update_tau`       | Proporci√≥n del soft update entre redes principales y redes objetivo    |\n| `target_update_period`    | Frecuencia de actualizaci√≥n del objetivo                               |\n| `gamma`                   | Factor de descuento para recompensas futuras                           |\n| `reward_scale_factor`     | Escala de las recompensas para mejorar la estabilidad num√©rica         |\n| `train_step_counter`      | Contador interno para rastrear los pasos de entrenamiento              |\n","metadata":{}},{"cell_type":"code","source":"# === Par√°metros de entrenamiento ===\nreplay_buffer_capacity = 100_000\nbatch_size = 64\ninitial_collect_steps = 1000\ncollect_steps_per_iteration = 1\nlog_interval = 200\nnum_iterations = 25_000  # Ajustado para que se entrene ~1 hora\n\n# === Pol√≠tica aleatoria para inicializar buffer ===\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n\n# === Replay Buffer ===\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_capacity\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:36.155732Z","iopub.execute_input":"2025-06-25T20:03:36.157313Z","iopub.status.idle":"2025-06-25T20:03:36.213345Z","shell.execute_reply.started":"2025-06-25T20:03:36.157257Z","shell.execute_reply":"2025-06-25T20:03:36.211675Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Esta parte define los hiperpar√°metros del proceso de entrenamiento:\n\nreplay_buffer_capacity: tama√±o m√°ximo del buffer de experiencia. Se almacenan hasta 100,000 transiciones.\n\nbatch_size: n√∫mero de muestras por batch durante el entrenamiento.\n\ninitial_collect_steps: pasos iniciales usando pol√≠tica aleatoria para llenar el buffer con datos diversos.\n\ncollect_steps_per_iteration: pasos que se recolectan del entorno por cada paso de entrenamiento.\n\nlog_interval: cada cu√°ntos pasos se imprime el log de p√©rdida.\n\nnum_iterations: total de pasos de entrenamiento. Aqu√≠ se ha aumentado a 25,000 para una mejor convergencia.\n\n","metadata":{}},{"cell_type":"code","source":"# === Funci√≥n para recolectar experiencia paso a paso ===\ndef collect_step(environment, policy, buffer):\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    next_time_step = environment.step(action_step.action)\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n    buffer.add_batch(traj)\n\n# === Rellenar el buffer con pol√≠tica aleatoria inicialmente ===\nfor _ in range(initial_collect_steps):\n    collect_step(train_env, random_policy, replay_buffer)\nprint(\"‚úÖ Buffer inicializado con pasos aleatorios.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:36.215504Z","iopub.execute_input":"2025-06-25T20:03:36.215903Z","iopub.status.idle":"2025-06-25T20:03:44.358821Z","shell.execute_reply.started":"2025-06-25T20:03:36.215871Z","shell.execute_reply":"2025-06-25T20:03:44.357034Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Antes de entrenar al agente, se utiliza una pol√≠tica aleatoria para recolectar las primeras experiencias. Esto evita que el agente empiece desde un buffer vac√≠o, lo cual podr√≠a causar errores o aprendizaje poco efectivo al principio.\n\n","metadata":{}},{"cell_type":"code","source":"# === Dataset desde buffer para entrenamiento ===\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3,\n    sample_batch_size=batch_size,\n    num_steps=2\n).prefetch(3)\n\niterator = iter(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:44.360354Z","iopub.execute_input":"2025-06-25T20:03:44.360757Z","iopub.status.idle":"2025-06-25T20:03:46.035044Z","shell.execute_reply.started":"2025-06-25T20:03:44.360727Z","shell.execute_reply":"2025-06-25T20:03:46.033492Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Convierte el ReplayBuffer en un dataset de TensorFlow, que se puede usar directamente para alimentar el proceso de entrenamiento del agente. Esto permite un entrenamiento eficiente, asincr√≥nico y en forma de flujo continuo de datos (streaming).","metadata":{}},{"cell_type":"code","source":"# === Entrenamiento principal ===\nreturns = []\nfor i in range(num_iterations):\n    # Colectar un paso con la pol√≠tica actual del agente\n    collect_step(train_env, agent.collect_policy, replay_buffer)\n\n    # Entrenar con un batch de datos\n    experience, _ = next(iterator)\n\n    #print(f\"üìê experience.action shape: {experience.action.shape}\")\n    #print(f\"üìê experience.observation shape: {experience.observation.shape}\")\n\n    # Entrenar conservando la forma [B, T, ...]\n    try:\n        train_loss = agent.train(experience).loss\n    except Exception as e:\n        print(f\"‚ùå Error en agent.train: {e}\")\n        break\n\n    # Log de progreso\n    if i % log_interval == 0:\n        print(f\"üîÅ Paso {i}: p√©rdida = {train_loss:.4f}\")\n\n    # Evaluaci√≥n (opcional, no renderiza)\n    if i % 1000 == 0:\n        total_return = 0.0\n        num_eval_episodes = 5\n        for _ in range(num_eval_episodes):\n            time_step = eval_env.reset()\n            episode_return = 0.0\n            while not time_step.is_last():\n                action_step = agent.policy.action(time_step)\n                time_step = eval_env.step(action_step.action)\n                episode_return += time_step.reward\n            total_return += episode_return\n        avg_return = total_return / num_eval_episodes\n        returns.append(avg_return)\n        print(f\"‚úÖ Recompensa promedio (eval) en paso {i}: {avg_return.numpy()[0]:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:03:46.036294Z","iopub.execute_input":"2025-06-25T20:03:46.036606Z","iopub.status.idle":"2025-06-25T20:51:38.312007Z","shell.execute_reply.started":"2025-06-25T20:03:46.036585Z","shell.execute_reply":"2025-06-25T20:51:38.310670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# === Gr√°fica de recompensa promedio ===\nplt.figure(figsize=(10, 5))\nsteps = np.arange(0, len(returns)) * 1000\nplt.plot(steps, [r.numpy()[0] for r in returns], label=\"Recompensa promedio\")\nplt.xlabel(\"Paso de entrenamiento\")\nplt.ylabel(\"Recompensa promedio\")\nplt.title(\"Evoluci√≥n de la recompensa del agente\")\nplt.grid(True)\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:51:38.313936Z","iopub.execute_input":"2025-06-25T20:51:38.314357Z","iopub.status.idle":"2025-06-25T20:51:38.766525Z","shell.execute_reply.started":"2025-06-25T20:51:38.314328Z","shell.execute_reply":"2025-06-25T20:51:38.765016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Esta funci√≥n genera un video de la pol√≠tica evaluada siguiendo estos pasos:\n\nInicializa el escritor de video con imageio.\n\nPara cada episodio:\n\nReinicia el entorno real (eval_py_env) y el entorno TF (eval_env).\n\nRenderiza el primer frame.\n\nMientras no termine el episodio:\n\nUsa la pol√≠tica (policy) para decidir la acci√≥n.\n\nAvanza un paso en el entorno TF.\n\nRenderiza el siguiente frame.\n\nAl finalizar, retorna el HTML del video incrustado.\n\nüîÅ Nota: Se ejecutan 3 episodios completos por defecto (num_episodes=3).","metadata":{}},{"cell_type":"code","source":"import base64\nimport IPython\nimport imageio\n\ndef embed_mp4(filename):\n    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n    video = open(filename, 'rb').read()\n    b64 = base64.b64encode(video)\n    tag = '''\n    <video width=\"640\" height=\"480\" controls>\n      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n    </video>'''.format(b64.decode())\n    return IPython.display.HTML(tag)\n\n\ndef create_policy_eval_video(policy, filename, num_episodes=3, fps=30):\n    \"\"\"Eval√∫a una pol√≠tica y guarda video en MP4.\"\"\"\n    filename = filename + \".mp4\"\n    with imageio.get_writer(filename, fps=fps) as video:\n        for _ in range(num_episodes):\n            time_step = eval_env.reset()\n            eval_py_env.reset()\n            frame = eval_py_env.render(mode=\"rgb_array\")\n            video.append_data(frame)\n            while not time_step.is_last():\n                action_step = policy.action(time_step)\n                time_step = eval_env.step(action_step.action)\n                frame = eval_py_env.render(mode=\"rgb_array\")\n                video.append_data(frame)\n    return embed_mp4(filename)\n\n# ‚úÖ Crear y mostrar video del agente entrenado\ncreate_policy_eval_video(agent.policy, \"trained-agent\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:51:38.770470Z","iopub.execute_input":"2025-06-25T20:51:38.770877Z","iopub.status.idle":"2025-06-25T20:51:53.676283Z","shell.execute_reply.started":"2025-06-25T20:51:38.770838Z","shell.execute_reply":"2025-06-25T20:51:53.674510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Crear y mostrar video del agente aleatorio\ncreate_policy_eval_video(random_policy, \"random-agent\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T20:51:53.678258Z","iopub.execute_input":"2025-06-25T20:51:53.679935Z","iopub.status.idle":"2025-06-25T20:52:02.659905Z","shell.execute_reply.started":"2025-06-25T20:51:53.679869Z","shell.execute_reply":"2025-06-25T20:52:02.658541Z"}},"outputs":[],"execution_count":null}]}